{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mchomak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mchomak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import pickle\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11250, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/df_1250.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined_text'] = df[['headers', 'sub_headers', 'text']].fillna('').apply(lambda x: ' '.join(x[x != '']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns= [\"url\", 'headers', 'sub_headers', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11250, 1) (11250, 1)\n"
     ]
    }
   ],
   "source": [
    "X = df[[\"combined_text\"]]\n",
    "y = df[[\"ID\"]]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    with open(f'{name}.pkl','wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Лемматизация считается 10000 лет и не высчитывается не знаю в чем проблема( помогите\n",
    "    # morph = MorphAnalyzer()\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "    # Очистка текста от всей пунткуации\n",
    "    text = text.translate(str.maketrans('', '', punctuation))\n",
    "\n",
    "    # Очистка текста от всех символов, кроме букв\n",
    "    text =  re.sub(r'[^а-яёА-ЯЁ]', ' ', text)\n",
    "\n",
    "    # нижний регистр\n",
    "    text = text.lower()\n",
    "\n",
    "    # токинизируем\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    # убираем стоп слова\n",
    "    filtered_words = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "            # filtered_words.append(morph.parse(word)[0].normal_form)\n",
    "\n",
    "    text = ' '.join(filtered_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"combined_text\"] = X[\"combined_text\"].apply(clean_text).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[[\"combined_text\"]], y[\"ID\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_w2v_vector(sentence, model, HIDDEN):\n",
    "    Sum = np.zeros(HIDDEN)\n",
    "    Count = 0\n",
    "\n",
    "    try:\n",
    "        words = sentence.split()\n",
    "    except TypeError:\n",
    "        words = []\n",
    "\n",
    "    for w in words:\n",
    "        if w in model.wv:\n",
    "            Sum += model.wv[w]\n",
    "            Count += 1\n",
    "\n",
    "    if Count == 0:\n",
    "        return Sum  # Возвращаем нулевой вектор, если нет слов в модели\n",
    "\n",
    "    return Sum / Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_processing(X, model, HIDDEN, y = None):\n",
    "    NewCols = ['col' + str(i) for i in range(HIDDEN)]\n",
    "    X['vectors'] = X[\"combined_text\"].map(lambda text: get_mean_w2v_vector(text, model=model, HIDDEN=HIDDEN))\n",
    "\n",
    "    Idx = []\n",
    "\n",
    "    for ix, row in X.iterrows():\n",
    "        if not isinstance(row['vectors'], np.ndarray):\n",
    "            Idx.append(ix)\n",
    "    \n",
    "    X.drop(index=Idx, inplace=True)\n",
    "\n",
    "    if y is not None:\n",
    "        y = y.drop(index=Idx)\n",
    "\n",
    "    X[NewCols] = pd.DataFrame(X['vectors'].tolist(), index=X.index)\n",
    "\n",
    "    X.drop([\"combined_text\",'vectors'], axis=1, inplace=True)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [row.split() for row in X_train[\"combined_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'vector_size': [50, 100, 200, 300],\n",
    "    'window': [2, 5, 10],\n",
    "    'min_count': [1, 2, 5, 10],\n",
    "    'sample': np.linspace(0.0001, 0.001, num=5),\n",
    "    'alpha': np.linspace(0.01, 0.05, num=5),\n",
    "    'min_alpha': np.linspace(0.0001, 0.001, num=5),\n",
    "    'negative': [5, 10, 20],\n",
    "    'workers': [4],  # Количество рабочих потоков для обучения модели\n",
    "}\n",
    "n_iter = 10  # Количество комбинаций для тестирования\n",
    "\n",
    "parameter_sampler = ParameterSampler(param_distributions, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, HIDDEN):\n",
    "    X_train, y_train = w2v_processing(X_train, model= model, HIDDEN= HIDDEN)\n",
    "    X_test, y_test = w2v_processing(X_test, y_test)\n",
    "\n",
    "    lg = LogisticRegression()\n",
    "    lg.fit(X_train, y_train)\n",
    "\n",
    "    lg_train_pred = lg.predict(X_train)\n",
    "    lg_test_pred = lg.predict(X_test)\n",
    "    \n",
    "    return accuracy_score(y_test, lg_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Either one of corpus_file or corpus_iterable value must be provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m parameter_sampler:\n\u001b[0;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences\u001b[38;5;241m=\u001b[39msent, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Оцените модель\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     score \u001b[38;5;241m=\u001b[39m evaluate_model(model, HIDDEN \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mvector_size)  \u001b[38;5;66;03m# X и y должны быть определены пользователем как данные для задачи классификации\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mchomak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\models\\word2vec.py:1046\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_training_sanity(epochs\u001b[38;5;241m=\u001b[39mepochs, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words)\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_corpus_sanity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1050\u001b[0m     msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m     ),\n\u001b[0;32m   1055\u001b[0m )\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss \u001b[38;5;241m=\u001b[39m compute_loss\n",
      "File \u001b[1;32mc:\\Users\\Mchomak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\models\\word2vec.py:1497\u001b[0m, in \u001b[0;36mWord2Vec._check_corpus_sanity\u001b[1;34m(self, corpus_iterable, corpus_file, passes)\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether the corpus parameters make sense.\"\"\"\u001b[39;00m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither one of corpus_file or corpus_iterable value must be provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoth corpus_file and corpus_iterable must not be provided at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Either one of corpus_file or corpus_iterable value must be provided"
     ]
    }
   ],
   "source": [
    "best_score = None\n",
    "best_model = None\n",
    "\n",
    "for params in parameter_sampler:\n",
    "    model = Word2Vec(sentences=sent, epochs=30, **params)\n",
    "    model.train(sentences=sent, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    # Оцените модель\n",
    "    score = evaluate_model(model, HIDDEN = model.vector_size)  # X и y должны быть определены пользователем как данные для задачи классификации\n",
    "\n",
    "    if best_score is None or score > best_score:\n",
    "        best_score = score\n",
    "        best_model = model\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Лучший результат: {best_score}, Лучшие параметры: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
